{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8c552a5",
   "metadata": {},
   "source": [
    "# NLP analysis\n",
    "\n",
    "```\n",
    "conda create --name nlp -c conda-forge python=3.10 jupyter pandas numpy matplotlib openpyxl nltk gensim pyldavis spacy scikit-learn kneed\n",
    "conda activate nlp \n",
    "## pip install bertopic\n",
    "## pip install git+https://github.com/boudinfl/pke.git \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e87faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you are running this for the first time on a new installation, uncomment below and run this cell\n",
    "## (This only needs to be run once.)\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "# import spacy\n",
    "# spacy.cli.download('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b589ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b3efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to autoreload <-- only necessary while coding/debugging\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import my code \n",
    "from NLPforISP import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0681f1cd",
   "metadata": {},
   "source": [
    "## Read in the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68559a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full data file with multiple sheets\n",
    "filename = 'data/ITP_CourseArtifacts_June 2021_END_of_Course_DeIDENTIFIED.xlsx'\n",
    "\n",
    "# sheet name for this analysis, containing responses to one question\n",
    "#sheet = 'Course Meta SelfEff'\n",
    "sheet = 'Course Meta App'\n",
    "\n",
    "df = pd.read_excel(filename, sheet)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36c0337",
   "metadata": {},
   "source": [
    "## Get the bigrams and trigrams and create bar charts of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16366eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add appropriate words that will be ignored in the analysis\n",
    "additional_stopwords = ['1', '2', 'one', 'two', 'etc']\n",
    "\n",
    "# get a string of the words contained in all the answers from this DataFrame\n",
    "string_of_answers = getStringOfWords(df, 1)\n",
    "\n",
    "# get the bigrams and trigrams\n",
    "bigrams = getNgrams(string_of_answers, 2, additional_stopwords = additional_stopwords)\n",
    "trigrams = getNgrams(string_of_answers, 3, additional_stopwords = additional_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a13508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a plot of the bigrams and trigrams\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "N = 20\n",
    "plotNgrams(bigrams, N, ax = ax1)\n",
    "plotNgrams(trigrams, N, ax = ax2)\n",
    "_ = ax1.set_title(str(N) + ' Most Frequently Occuring Bigrams')\n",
    "_ = ax2.set_title(str(N) + ' Most Frequently Occuring Trigrams')\n",
    "plt.subplots_adjust(wspace = 0.6, left = 0.15, right = 0.99, top = 0.95, bottom = 0.07)\n",
    "\n",
    "f.savefig('ngrams_' + sheet.replace(' ','') + '.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ee5820",
   "metadata": {},
   "source": [
    "## Topic modeling\n",
    "\n",
    "Using NLTK + gensim,  Latent Dirichlet Allocation (LDA) algorithm, which uses unsupervised learning to extract the main topics (i.e., a set of words) that occur in a collection of text samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927c9c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the topic model (which also generates a \"dictionary\" and a \"bag of words\")\n",
    "dictionary, bow_corpus, lda_model, perplexity, coherence = runLDATopicModel(df, 1, 5, workers = 6, \n",
    "    additional_stopwords = additional_stopwords, no_below = 15, no_above = 1, keep_n = int(1e5),\n",
    "    random_state = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6294899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dictionary\n",
    "printDictionary(dictionary, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3b5627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the bag of words\n",
    "printBagOfWords(dictionary, bow_corpus, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a7b6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the topic model\n",
    "printLDATopicModel(lda_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f47dfaa",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "Run a series of LDA models and plot the coherence and perplexity scores to try to identify the optimal number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756179c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = np.arange(10) + 1\n",
    "dictionary, bow_corpus, lda_model, perplexity, coherence = runLDATopicModel(df, 1, num_topics, workers = 6, \n",
    "    additional_stopwords = additional_stopwords, no_below = 15, no_above = 1, keep_n = int(1e5),\n",
    "    random_state = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a0a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the index of the best model by selecting the maximum coherence score\n",
    "# choose the 'c_v' measure of coherence for this\n",
    "\n",
    "best_index = np.argmax(coherence['c_v'])\n",
    "num_topics[best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18077003",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the results\n",
    "# higher coherence is better\n",
    "# lower perplexity is better\n",
    "\n",
    "f, (ax1, ax2) = plotLDAMetrics(num_topics, coherence, perplexity, best_index)\n",
    "f.savefig('metrics_' + sheet.replace(' ','') + '.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13965aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the probabilities for each answer being in each topic\n",
    "df_p = getLDAProbabilities(lda_model[best_index], bow_corpus, df, 1)\n",
    "df_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4549b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a KDE of the probability distributions for each topic\n",
    "f, ax = plotTopLDAProbabilitiesKDE(df_p)#, bw_method = 0.3)\n",
    "f.savefig('probabilities_' + sheet.replace(' ','') + '.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab068974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get summary information about the topics\n",
    "df_p.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92319578",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print the answers that have the maximum probability for each topic\n",
    "printBestLDATopicSentences(df_p, dictionary, lda_model[best_index], n_answers = 20, n_sentences = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be4b9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9d4b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d6802f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e622e195",
   "metadata": {},
   "source": [
    "## Visualization using pyLDAvis\n",
    "\n",
    "- https://nbviewer.org/github/bmabey/hacker_news_topic_modelling/blob/master/HN%20Topic%20Model%20Talk.ipynb\n",
    "- https://github.com/bmabey/pyLDAvis\n",
    "- https://nbviewer.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb\n",
    "\n",
    "Most of the visualization is self expanatory, but the slider to adjust the \"relevant metric\" takes some reading. \n",
    "From here: https://we1s.ucsb.edu/research/we1s-tools-and-software/topic-model-observatory/tmo-guide/tmo-guide-pyldavis/\n",
    "\n",
    "\"A “relevance metric” slider scale at the top of the right panel controls how the words for a topic are sorted. As defined in the article by Sievert and Shirley (the creators of LDAvis, on which pyLDAvis is based), “relevance” combines two different ways of thinking about the degree to which a word is associated with a topic:\n",
    "\n",
    "On the one hand, we can think of a word as highly associated with a topic if its frequency in that topic is high. By default the lambda (λ) value in the slider is set to “1,” which sorts words by their frequency in the topic (i.e., by the length of their red bars).\n",
    "\n",
    "On the other hand, we can think of a word as highly associated with a topic if its “lift” is high. “Lift”–a term that Sievert and Shirley borrow from research on topic models by others–means basically how much a word’s frequency sticks out in a topic above the baseline of its overall frequency in the model (i.e., the “the ratio of a term’s probability within a topic to its marginal probability across the corpus,” or the ratio between its red bar and blue bar).\n",
    "\n",
    "By default, pyLDAvis is set for λ = 1, which sorts words just by their frequency within the specific topic (by their red bars).  By contrast, setting λ = 0 words sorts words by their “lift. This means that words whose red bars are nearly as long as their blue bars will be sorted at the top. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124c5964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baf6cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3af8ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Note: I chose the best index from the lda_models array while plotting the coherence and perplexity metrics\n",
    "pyLDAvis.gensim_models.prepare(lda_model[best_index], bow_corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992ff977",
   "metadata": {},
   "source": [
    "## Term Frequency – Inverse Document Frequency (TF-IDF) analysis\n",
    "\n",
    "TF-IDF (using sci-kit learn’s TfidfVectorizer) measures the frequency of a word in a document and compares it to the frequencies of all words in the text to assign it a weighted score of importance.\n",
    "\n",
    "https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/03-TF-IDF-Scikit-Learn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d82d4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1ac57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_stopwords = ['1', '2', 'one', 'two', 'etc', 'also']\n",
    "dictionary, bow_corpus, processed_answers = getBagOfWords(df, 1,  additional_stopwords = additional_stopwords)\n",
    "processed_answers_list = [' '.join(x) for x in processed_answers]\n",
    "vocab = [v for k, v in dictionary.iteritems()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fdca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF (word level)\"\"\n",
    "vectorizer = TfidfVectorizer(analyzer = 'word', ngram_range = (1,2), min_df = 0.01, vocabulary = vocab)\n",
    "tfidf_vector = vectorizer.fit_transform(processed_answers_list)\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), columns = vectorizer.get_feature_names_out())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3fae24",
   "metadata": {},
   "source": [
    "# k-means\n",
    "\n",
    "- using TF-IDF vectorizor (in sklearn) from above and then sklearn kmeans\n",
    "- this method requires knowing the number of clusters you want\n",
    "- https://towardsdatascience.com/clustering-product-names-with-python-part-1-f9418f8705c8\n",
    "- https://towardsdatascience.com/clustering-product-names-with-python-part-2-648cc54ca2ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2ae03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb076ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test increments of clusters using elbow method\n",
    "sse = {}\n",
    "for k in np.arange(2,20):\n",
    "    kmeans = KMeans(n_clusters = k, max_iter = 1000).fit(tfidf_vector)\n",
    "    sse[k] = kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab78dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the elbow in this curve\n",
    "x = list(sse.keys())\n",
    "y = list(sse.values())\n",
    "kneedle = KneeLocator(x, y, S = 0.0, curve = \"convex\", direction = \"decreasing\", online = False, interp_method = \"interp1d\")\n",
    "\n",
    "print(kneedle.knee, kneedle.elbow)\n",
    "\n",
    "f,ax = plt.subplots()\n",
    "ax.axvline(kneedle.elbow, linestyle = '--', color = 'gray')\n",
    "ax.plot(x,y, '-o', color = 'k')\n",
    "ax.set_xlabel('number of clusters')\n",
    "ax.set_ylabel('SSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c663fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = kneedle.elbow\n",
    "\n",
    "km_model = KMeans(n_clusters = n_clusters)\n",
    "km_model.fit(tfidf_vector)\n",
    "\n",
    "result = pd.concat([df, \n",
    "                    pd.DataFrame(tfidf_vector.toarray(),\n",
    "                                 columns = vectorizer.get_feature_names_out()\n",
    "                                )\n",
    "                   ],axis=1)\n",
    "\n",
    "result['cluster'] = km_model.predict(tfidf_vector)\n",
    "\n",
    "column_number = 1\n",
    "num_words_to_keep = 5\n",
    "\n",
    "# Label each cluster with the word(s) that all of its entries have in common\n",
    "clusters = result['cluster'].unique()\n",
    "labels = []\n",
    "for i in range(len(clusters)):\n",
    "    subset = result[result['cluster'] == clusters[i]]\n",
    "    exclude = [result.columns[j] for j in range(column_number + 1)] + ['cluster']\n",
    "    subset_words = subset.drop(exclude, axis = 1)\n",
    "\n",
    "    # count the number of times each word appears and take the top N\n",
    "    count = subset_words.astype(bool).sum(axis = 0).sort_values(ascending = False)\n",
    "    words = ' '.join(count[0:num_words_to_keep].index)\n",
    "    labels.append(words)\n",
    "\n",
    "labels_table = pd.DataFrame(zip(clusters,labels), columns=['cluster','label']).sort_values('cluster')\n",
    "# result_labelled = pd.merge(result,labels_table,on = 'cluster',how = 'left')\n",
    "\n",
    "\n",
    "labels_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8405d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sentences from the closest answers to the cluster\n",
    "dist = km_model.transform(tfidf_vector)\n",
    "\n",
    "# get a list of rows to remove that have <= 1 word in the matrix\n",
    "row_sums = tfidf_vector.toarray().astype(bool).sum(axis = 1)\n",
    "result_prune = result[row_sums > 1]\n",
    "dist_prune = dist[row_sums > 1]\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "n_answers = 10\n",
    "n_sentences = 2\n",
    "\n",
    "for i in range(len(clusters)):\n",
    "    print(f'********** Cluster {i} **********')\n",
    "    print(labels_table.loc[labels_table['cluster'] == i]['label'].values)\n",
    "    print('\\n')\n",
    "    \n",
    "    nearest_answer_indices = np.argsort(dist_prune[:,i])[:n_answers]\n",
    "    nearest_answers = result_prune.iloc[nearest_answer_indices]\n",
    "    # print(nearest_answers['cluster'])\n",
    "    \n",
    "    # combine these into one long text\n",
    "    combined_answers = nearest_answers[nearest_answers.columns[column_number]].str.cat(sep=' ')\n",
    "    \n",
    "    # split this into sentences\n",
    "    doc = nlp(combined_answers)\n",
    "    sentences = np.array([s.text for s in doc.sents])\n",
    "    sentences_processed = []\n",
    "    for s in doc.sents:\n",
    "        text = preprocess(s.text, additional_stopwords = additional_stopwords)\n",
    "        sentences_processed.append(' '.join(text))\n",
    "    \n",
    "    # get a new TF-IDF vector for each of these answers\n",
    "    tfidf_vector_sentences = vectorizer.fit_transform(sentences_processed)\n",
    "    \n",
    "    # get the distances\n",
    "    dist_sentences = km_model.transform(tfidf_vector_sentences)\n",
    "\n",
    "    # prune as above\n",
    "    row_sums = tfidf_vector_sentences.toarray().astype(bool).sum(axis = 1)\n",
    "    sentences_prune = sentences[row_sums > 1]\n",
    "    dist_sentences_prune = dist_sentences[row_sums > 1]\n",
    "\n",
    "    # get the nearest sentences\n",
    "    nearest_sentence_indices = np.argsort(dist_sentences_prune[:,i])[:n_sentences]\n",
    "    nearest_sentences = sentences_prune[nearest_sentence_indices]\n",
    "    nearest_distances = dist_sentences_prune[:,i][nearest_sentence_indices]\n",
    "    print(f'Most relevant {n_sentences} sentence(s) from the top {n_answers} answers:\\n')\n",
    "        \n",
    "    for (s,d) in zip(nearest_sentences, nearest_distances):\n",
    "        print(f'Distance from centroid = {d:.4}')\n",
    "        print(s.strip(),'\\n')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b1b58",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename = 'data/ITP_CourseArtifacts_June 2021_END_of_Course_DeIDENTIFIED.xlsx'\n",
    "num_topics = np.arange(2,20)\n",
    "results1_kmeans = runNLPPipeline(filename, sheet = 'Course Meta App', column_number = 1, num_topics = num_topics,\n",
    "    n_answers = 20, n_sentences = 3, tfidf_ngram_range = (1,2), tfidf_min_df = 0.01,\n",
    "    run_lda = False, run_lsi = False, run_ngrams = False,\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297b4e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21f34c99",
   "metadata": {},
   "source": [
    "#  TODO\n",
    "\n",
    "\n",
    "## Try Mallet LDA?\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/ <-- this also contains some great additional steps to check out\n",
    "\n",
    "Following steps from here : https://radimrehurek.com/gensim_3.8.3/models/wrappers/ldamallet.html\n",
    "\n",
    "(Working in WSL to compile the Mallet code.)\n",
    "\n",
    "```\n",
    "sudo apt update\n",
    "sudo apt-get install default-jdk\n",
    "git clone https://github.com/mimno/Mallet.git\n",
    "cd Mallet/\n",
    "ant\n",
    "```\n",
    "\n",
    "But this doesn't exist in gensim anymore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84983a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_mallet_binary = \"/c/Users/ageller/NUIT/projects/BennettGoldberg/Mallet/bin/mallet\"\n",
    "\n",
    "dictionary, bow_corpus, processed_answers = getBagOfWords(df, 1,  additional_stopwords = additional_stopwords, no_below = 15, no_above = 1, keep_n = int(1e5))\n",
    "\n",
    "model = gensim.models.wrappers.LdaMallet(path_to_mallet_binary, corpus = bow_corpus, num_topics = 5, \n",
    "                                         id2word = dictionary)\n",
    "vector = model[common_corpus[0]]  # LDA topics of a documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072469ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67a88ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92da058f",
   "metadata": {},
   "source": [
    "# Try PKE (Python Keyphrase Extraction)\n",
    "\n",
    "- https://medium.com/nlplanet/two-minutes-nlp-keyword-and-keyphrase-extraction-with-pke-5a0260e75f3e\n",
    "- https://github.com/boudinfl/pke (and see linked Colab notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ef62ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65c1156",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_of_answers = getStringOfWords(df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4a9eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a keyphrase extraction model, here TFxIDF\n",
    "extractor = pke.unsupervised.TfIdf()  \n",
    "\n",
    "# load the content of the document  (str or spacy Doc)\n",
    "extractor.load_document(input = string_of_answers)   \n",
    "\n",
    "# identify keyphrase candidates\n",
    "extractor.candidate_selection()    \n",
    "\n",
    "# weight keyphrase candidates\n",
    "extractor.candidate_weighting()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b03bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the 10-best candidates as keyphrases\n",
    "keyphrases = extractor.get_n_best(n = 10)          \n",
    "keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefff573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize keyphrase extraction model, here TopicRank\n",
    "extractor = pke.unsupervised.TopicRank()\n",
    "\n",
    "# load text\n",
    "extractor.load_document(input = string_of_answers, language = 'en')\n",
    "\n",
    "# keyphrase candidate selection, in the case of TopicRank: sequences of nouns\n",
    "# and adjectives (i.e. `(Noun|Adj)*`)\n",
    "extractor.candidate_selection()\n",
    "\n",
    "# candidate weighting, in the case of TopicRank: using a random walk algorithm\n",
    "# In TopicRank, candidate weighting is a three-step process:\n",
    "#  1. candidate clustering (grouping keyphrase candidates into topics)\n",
    "#  2. graph construction (building a complete-weighted-graph of topics)\n",
    "#  3. rank topics (nodes) using a random walk algorithm\n",
    "extractor.candidate_weighting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99496662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each keyphrase candidate\n",
    "for i, candidate in enumerate(extractor.candidates):\n",
    "    \n",
    "    # print out the candidate id, its stemmed form \n",
    "    print(\"candidate {}: {} (stemmed form)\".format(i, candidate))\n",
    "    \n",
    "    # print out the surface forms of the candidate\n",
    "    print(\" - surface forms:\", [ \" \".join(u) for u in extractor.candidates[candidate].surface_forms])\n",
    "    \n",
    "    # print out the corresponding offsets\n",
    "    print(\" - offsets:\", extractor.candidates[candidate].offsets)\n",
    "    \n",
    "    # print out the corresponding sentence ids\n",
    "    print(\" - sentence_ids:\", extractor.candidates[candidate].sentence_ids)\n",
    "    \n",
    "    # print out the corresponding PoS patterns\n",
    "    print(\" - pos_patterns:\", extractor.candidates[candidate].pos_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c5c64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each topic of the document\n",
    "for i, topic in enumerate(extractor.topics):\n",
    "    \n",
    "    # print out the topic id and the candidates it groups together\n",
    "    print(\"topic {}: {} \".format(i, ';'.join(topic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572da3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of the weights\n",
    "f,ax = plt.subplots()\n",
    "ax.hist(extractor._w.values(), bins= 100)\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d156b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let have a look at the graph-based representation of the document\n",
    "#\n",
    "# here, nodes are topics, edges between topics are weighted according to \n",
    "# the strength of their semantic relation measured by the reciprocal distances\n",
    "# between the offset positions of the candidate keyphrases\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "# set the labels as list of candidates for each topic\n",
    "labels = {i: ';'.join(topic) for i, topic in enumerate(extractor.topics)}\n",
    "\n",
    "# set the weights of the edges\n",
    "edge_weights = [extractor.graph[u][v]['weight'] for u,v in extractor.graph.edges()]\n",
    "\n",
    "# set the weights of the nodes (topic weights are stored in _w attribute)\n",
    "sizes = [10e3*extractor._w[i] for i, topic in enumerate(extractor.topics)]\n",
    "\n",
    "# draw the graph\n",
    "nx.draw_shell(extractor.graph,\n",
    "              #with_labels = True, labels = labels, \n",
    "              #width = edge_weights, \n",
    "              node_size = sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c059ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-best selection, keyphrases contains the 10 highest scored candidates as\n",
    "# (keyphrase, score) tuples\n",
    "keyphrases = extractor.get_n_best(n = 10)\n",
    "\n",
    "# for each of the best candidates\n",
    "for i, (candidate, score) in enumerate(keyphrases):\n",
    "    \n",
    "    # print out the its rank, phrase and score\n",
    "    print(\"rank {}: {} ({})\".format(i, candidate, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43ea821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. create a YAKE extractor.\n",
    "extractor = pke.unsupervised.YAKE()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input = string_of_answers, language = 'en', normalization = None)\n",
    "\n",
    "# 3. select {1-3}-grams not containing punctuation marks and not\n",
    "#    beginning/ending with a stopword as candidates.\n",
    "extractor.candidate_selection(n = 3)\n",
    "\n",
    "# 4. weight the candidates using YAKE weighting scheme, a window (in\n",
    "#    words) for computing left/right contexts can be specified.\n",
    "window = 2\n",
    "use_stems = False # use stems instead of words for weighting\n",
    "extractor.candidate_weighting(window = window, use_stems = use_stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a006a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. get the 10-highest scored candidates as keyphrases.\n",
    "#    redundant keyphrases are removed from the output using levenshtein\n",
    "#    distance and a threshold.\n",
    "threshold = 0.8\n",
    "keyphrases = extractor.get_n_best(n = 10, threshold = threshold)\n",
    "\n",
    "keyphrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d3e11b",
   "metadata": {},
   "source": [
    "# LSI model using gensim\n",
    "- https://medium.com/@zeina.thabet/topic-modeling-with-lsi-lda-and-document-clustering-with-carrot2-part-1-5b1fbec737f6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba84ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = np.arange(12) + 3\n",
    "dictionary, bow_corpus, models, coherence = runLSITopicModel(df, column_number = 1, num_topics = num_topics, \n",
    "                                                             random_seed = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566e6a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the index of the best model by selecting the maximum coherence score\n",
    "coherence_method = 'c_v'\n",
    "best_index = np.argmax(coherence[coherence_method])\n",
    "print(f'  -- The best model has {num_topics[best_index]} topics, using the \"{coherence_method}\" coherence method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0971a9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plotLSIMetrics(num_topics, coherence, best_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa752837",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model = models[best_index]\n",
    "lsi_model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f6abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the Euclidean distances between each topic and a given answer\n",
    "df_d = getLSIDistances(lsi_model, bow_corpus, df)\n",
    "df_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dbbf5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# uses gensim to get the sum of the vector coefficients for each answer\n",
    "# I'm not sure how this is helpful, unless there are only 2 topics\n",
    "df_v = getLSIVectors(lsi_model, bow_corpus, df)\n",
    "df_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8297f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTopLSIVectorsKDE(df_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6086a408",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTopLSIVectorsKDE(df_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae7b2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_vectors = lsi_model.get_topics()\n",
    "print(topic_vectors.shape)\n",
    "print(len(bow_corpus), len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e21d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_vectors[0])\n",
    "print(sum(topic_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622be4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proving that the lsi_model[bow] is the same as summing the coefficients for the words in bow for a given topic\n",
    "text = df[df.columns[1]][0]\n",
    "text_list = preprocess(text)\n",
    "bow = dictionary.doc2bow(text_list)\n",
    "\n",
    "topic_index = 1\n",
    "\n",
    "x = 0\n",
    "for w in bow:\n",
    "    val = topic_vectors[topic_index][w[0]]*w[1]\n",
    "    print(val)\n",
    "    x += val\n",
    "print('sum',x)\n",
    "print('check',lsi_model[bow][topic_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9178d3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvector = topic_vectors[1]\n",
    "sentence_vector = np.zeros(len(tvector))\n",
    "#sentence_vector[0] = tvector[0]\n",
    "print(np.linalg.norm(tvector - sentence_vector))\n",
    "print(np.dot(tvector.T, sentence_vector))\n",
    "text = preprocess(\"Improving syllabus resources to be more welcoming and accommodating.  Using formative assessments to gauge classroom climate.\")\n",
    "bow = dictionary.doc2bow(text)\n",
    "print(bow)\n",
    "for i,w in enumerate(bow):\n",
    "    sentence_vector[w[0]] = tvector[w[0]]\n",
    "print(np.linalg.norm(tvector - sentence_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd79b5d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# this doesn't seem to be working correctly\n",
    "printBestLSITopicSentences(df_d, dictionary, lsi_model)#, show_answers = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238fa10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"(1) Designing content and presentation to better address student's interests and goals.\"\n",
    "preprocess(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d018251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the dot product to find the distance between the sentence and topic vectors\n",
    "answer_vector = np.zeros(len(topic_vectors[topic_index]))\n",
    "for i,w in enumerate(bow):\n",
    "    answer_vector[i] = topic_vectors[topic_index][w[0]]\n",
    "dist = np.dot(answer_vector, topic_vectors[topic_index])\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d40fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model[bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff205425",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_vector = [(i, v) for i, v in enumerate(topic_vectors[topic_index])]\n",
    "print(use_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaa6c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = gensim.similarities.MatrixSimilarity(topic_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0f622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df[df.columns[1]][0]\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "\n",
    "available_words = np.array(list(dictionary.items()))[:,1]\n",
    "\n",
    "sentence_strength = {}\n",
    "for sentence in doc.sents:\n",
    "    for word in sentence:\n",
    "        if (word.text in available_words):\n",
    "            print(dictionary.doc2bow)\n",
    "#             prob = lda_model.get_term_topics(word.text)[topic_number][1]\n",
    "#             if sentence in sentence_strength.keys():\n",
    "#                 sentence_strength[sentence] += prob\n",
    "#             else:\n",
    "#                 sentence_strength[sentence] = prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226872b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281a49a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a41a41f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca01ebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary, bow_corpus, processed_answers = getBagOfWords(df, 1)\n",
    "lsimodel = gensim.models.LsiModel(corpus = bow_corpus, num_topics = 10, id2word = dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb51ae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the u matrix\n",
    "u_df = pd.DataFrame(data= lsimodel.projection.u)\n",
    "# sort by column 0\n",
    "u_df.sort_values(axis= 0, by= 0, ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be6649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the s matrix\n",
    "s_df = pd.DataFrame(data= lsimodel.projection.s)\n",
    "s_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd201c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsimodel.show_topics(num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e57e788",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsitopics = [[word for word, prob in topic]\n",
    "             for topicid, topic in lsimodel.show_topics(formatted = False)]\n",
    "\n",
    "coherence_model_lsi = gensim.models.CoherenceModel(model = lsimodel, texts = processed_answers, \n",
    "                                                   dictionary = dictionary, coherence='c_v', topics = lsitopics)\n",
    "coherence_lsi = coherence_model_lsi.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a683c5",
   "metadata": {},
   "source": [
    "# BERTopic\n",
    "\n",
    "- https://towardsdatascience.com/meet-bertopic-berts-cousin-for-advanced-topic-modeling-ea5bf0b7faa3\n",
    "- https://github.com/MaartenGr/BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43e8981",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420290ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c60cc1f2",
   "metadata": {},
   "source": [
    "# Use Bennett's categories to put answers into buckets\n",
    "\n",
    "Look at the `ISTP Codebook.xslx` file for the words that define each category.\n",
    "\n",
    "- **Identity**\n",
    "    - Instructor Identity\n",
    "    - Student Identity\n",
    "    - Positionality\n",
    "- **Course Structure**\n",
    "    - Overall course design\n",
    "    - Implementation\n",
    "- **Course Climate**\n",
    "    - Creating an inclusive course climate\n",
    "    - Conflict management\n",
    "- **Concept Knowledge**\n",
    "    - Understanding inclusive practices\n",
    "    - Interpersonal; sharing knowledge\n",
    "- **Planned Implementation**\n",
    "    - Synthesis of knowledge\n",
    "    - Reasons for hesitancies or resistance\n",
    "\n",
    "\n",
    "I also want to know for a given answer in a category if the response is already increased confidence or still gaining confidence.  I think I may need to split each answer into sentences and do the analysis on each sentence..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d285368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "additional_stopwords = ['']\n",
    "wlen = 3\n",
    "stem = True\n",
    "no_below = 1\n",
    "no_above = 1\n",
    "keep_n = int(1e10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0525e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73676d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the categories\n",
    "# I created this file from the ISTP Codebook.xlsx file\n",
    "with open('data/category_words.json') as json_file:\n",
    "    categories_all = json.load(json_file)\n",
    "\n",
    "\n",
    "#categories_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7414cee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, collapse the subcategories\n",
    "categories_primary = {}\n",
    "for k1 in categories_all:\n",
    "    categories_primary[k1] = categories_all[k1]['text']\n",
    "    for k2 in categories_all[k1]['subcategories']:\n",
    "        categories_primary[k1] += ' ' + categories_all[k1]['subcategories'][k2]\n",
    "\n",
    "#categories_primary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017941c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_categories = {}\n",
    "for key in categories_primary:\n",
    "    processed_categories[key] = preprocess(\n",
    "        categories_primary[key], additional_stopwords = additional_stopwords, wlen = wlen, stem = stem\n",
    "    )\n",
    "categories_names = np.array(list(processed_categories.keys()))\n",
    "#processed_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b1fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to eliminate overlapping words between categories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e721e006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full data file with multiple sheets\n",
    "filename = 'data/ITP_CourseArtifacts_June 2021_END_of_Course_DeIDENTIFIED.xlsx'\n",
    "\n",
    "# sheet name for this analysis, containing responses to one question\n",
    "sheet = 'Course Meta SelfEff'\n",
    "\n",
    "df = pd.read_excel(filename, sheet)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff5cc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each answer, try to divide into chunks of \"gained confidence\" and \"still gaining confidence\"\n",
    "# this really should have been split into two questions!\n",
    "results_df = df.copy()\n",
    "\n",
    "gained_list = ['' for x in range(len(df))]\n",
    "gaining_list = ['' for x in range(len(df))]\n",
    "neither_list = ['' for x in range(len(df))]\n",
    "for i, row in df.iterrows():\n",
    "    n_key = 0\n",
    "    answer = re.sub(r'[^\\w\\s]', '', (unicodedata.normalize('NFKD', row[columns[1]])\n",
    "        .encode('ascii', 'ignore')\n",
    "        .decode('utf-8', 'ignore')\n",
    "        .lower()))\n",
    "\n",
    "    words = np.array(answer.split(' '))\n",
    "    igained_start = -1\n",
    "    igaining_start = -1\n",
    "\n",
    "    gained = []\n",
    "    gaining = []\n",
    "    neither = []\n",
    "    for k, w in enumerate(words):\n",
    "        if (k < len(words) - 2):\n",
    "            if ( \n",
    "                    (w == 'increasing') or \n",
    "                    (w == 'gaining') or \n",
    "                    (w + words[k + 1] == 'lessconfident') or \n",
    "                    (w + words[k + 1] == 'notconfident') or \n",
    "                    (w + words[k + 1] == 'amstill') or \n",
    "                    (w + words[k + 1] == 'imstill') or\n",
    "                    (w + words[k + 1] == 'stillneed')\n",
    "            ):\n",
    "                if (igaining_start < 0): \n",
    "                    igaining_start = k\n",
    "                if (igained_start >= 0):\n",
    "                    #gained += ' '.join(words[igained_start:k])\n",
    "                    gained.extend([x + igained_start for x in range(k - igained_start)])\n",
    "                    igained_start = -1\n",
    "                    \n",
    "            elif ( (\n",
    "                    (w == 'increased') or \n",
    "                    (w == 'gained') or \n",
    "                    (w == 'confident') or \n",
    "                    (w + words[k + 1] == 'nowfeel')\n",
    "                ) and (k - igaining_start > 3 or igaining_start == -1)\n",
    "            ): \n",
    "                if (igained_start < 0):\n",
    "                    igained_start = k\n",
    "                if (igaining_start >= 0):\n",
    "                    #gaining += ' '.join(words[igaining_start:k])\n",
    "                    gaining.extend([x + igaining_start for x in range(k - igaining_start)])\n",
    "                    igaining_start = -1\n",
    "\n",
    "\n",
    "                    \n",
    "    if (igained_start > 0):\n",
    "        #gained += ' '.join(words[igained_start:len(words)])\n",
    "        gained.extend([x + igained_start for x in range(len(words) - igained_start)])\n",
    "\n",
    "    if (igaining_start > 0):\n",
    "        #gaining += ' '.join(words[igaining_start:len(words)])        \n",
    "        gaining.extend([x + igaining_start for x in range(len(words) - igaining_start)])\n",
    "\n",
    "    for x in range(len(words)):\n",
    "        if (x not in gained and x not in gaining):\n",
    "            neither.append(x)\n",
    "\n",
    "    if (len(gained) > 0):\n",
    "        gained_list[i] = ' '.join(words[gained])\n",
    "    if (len(gaining) > 0):\n",
    "        gaining_list[i] = ' '.join(words[gaining])\n",
    "    if (len(neither) > 0):\n",
    "        neither_list[i] = ' '.join(words[neither])\n",
    "\n",
    "    #print(i)\n",
    "    #print(answer + '\\n')\n",
    "    #print('GAINED', gained_list[i], '\\n')\n",
    "    #print('GAINING', gaining_list[i], '\\n')\n",
    "    #print('NEITHER', neither_list[i], '\\n')\n",
    "    \n",
    "results_df['gained'] = gained_list\n",
    "results_df['gaining'] = gaining_list\n",
    "results_df['neither'] = neither_list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88ae430",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f28403",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = results_df.columns.to_list()\n",
    "cols = [2,3] #gained, gaining\n",
    "\n",
    "for column_number in cols:\n",
    "    column_name = columns[column_number]\n",
    "    \n",
    "    # first create a bag of words for each survey answer\n",
    "    dictionary, bow_corpus, processed_answers = getBagOfWords(\n",
    "        results_df, column_number,  additional_stopwords = additional_stopwords, \n",
    "        wlen = wlen, stem = stem, no_below = no_below, no_above = no_above, \n",
    "        keep_n = keep_n\n",
    "    )\n",
    "\n",
    "    # loop through and count the words\n",
    "    # how do I normalize this to get a scale that will allow me to identify categories that the answers fall into\n",
    "    # I could divide by the number of words in the answer, but what if the answer is relevant to multiple categories?\n",
    "    # I will add a column that has the number of words in the answer so that I can use that later\n",
    "    categories_scores = np.zeros((len(df), len(processed_categories.keys())))\n",
    "    for i, a in enumerate(processed_answers):\n",
    "        for j, key in enumerate(categories_names):\n",
    "            for w in processed_categories[key]:\n",
    "                if (w in a):\n",
    "                    categories_scores[i,j] += 1/len(a)\n",
    "\n",
    "    # maybe I can keep track of some kind of maximum density of words within a group based on a rolling window?\n",
    "    window = 10 # this is meant to represent some average sentence length in the answers (I could calculate that)\n",
    "    categories_density = np.zeros((len(df), len(processed_categories.keys())))\n",
    "    for i, a in enumerate(processed_answers):\n",
    "        for j, key in enumerate(categories_names):\n",
    "            density = 0\n",
    "            wuse = np.min([window, len(a)])\n",
    "            for k, wa in enumerate(a):\n",
    "                if ((len(a) - k) >= wuse):\n",
    "                    wlist = a[k:(k + wuse)]\n",
    "                    d = 0\n",
    "                    for w in processed_categories[key]:\n",
    "                        if (w in wlist):\n",
    "                            d += 1/len(wlist)\n",
    "                    density = np.max([density, d])\n",
    "            categories_density[i,j] = density\n",
    "\n",
    "    # add these counts to the dataframe\n",
    "    for j, key in enumerate(categories_names):\n",
    "        results_df[key + ' ' + column_name] = categories_scores[:, j]\n",
    "    for j, key in enumerate(categories_names):\n",
    "        results_df[key + ' ' + column_name + ' density'] = categories_density[:, j]\n",
    "\n",
    "\n",
    "# TODO: impose some threshold to place in \"Not specific\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3092a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570209a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "for j, key in enumerate(categories_names):\n",
    "    ax[0].scatter(results_df[key + ' gained'], results_df[key + ' gained density'], label = key)\n",
    "    ax[1].scatter(results_df[key + ' gaining'], results_df[key + ' gaining density'], label = key)\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('fraction gained')\n",
    "ax[0].set_ylabel('density gained')\n",
    "ax[1].set_xlabel('fraction gaining')\n",
    "ax[1].set_ylabel('density gaining')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c280bfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "full_array = []\n",
    "for j, key in enumerate(categories_names):\n",
    "    full_array.extend(results_df[key + ' gained'].to_list())\n",
    "    ax[0].hist(results_df[key + ' gained'], label = key, alpha = 0.5, color = colors[j], bins = 20)\n",
    "    ax[0].hist(results_df[key + ' gained'], histtype = 'step', color = colors[j], linewidth = 2, bins = 20)\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('fraction')\n",
    "ax[0].set_ylabel('N')\n",
    "\n",
    "full_array = np.array(full_array)\n",
    "full_array = full_array[np.where(np.array(full_array) > 0)]\n",
    "\n",
    "ax[1].hist(full_array, bins = 20)\n",
    "median_fraction_gained, one_sig_fraction_gained, three_sig_fraction_gained = np.percentile(full_array, [50, 84.1, 99.8])\n",
    "ax[1].axvline(median_fraction_gained, color = colors[1])\n",
    "ax[1].axvline(one_sig_fraction_gained, linestyle = 'dashed', color = colors[1])\n",
    "ax[1].axvline(three_sig_fraction_gained, linestyle = 'dotted', color = colors[1])\n",
    "print(median_fraction_gained, one_sig_fraction_gained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586c90ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "full_array = []\n",
    "for j, key in enumerate(categories_names):\n",
    "    full_array.extend(results_df[key + ' gaining'].to_list())\n",
    "    ax[0].hist(results_df[key + ' gaining'], label = key, alpha = 0.5, color = colors[j], bins = 20)\n",
    "    ax[0].hist(results_df[key + ' gaining'], histtype = 'step', color = colors[j], linewidth = 2, bins = 20)\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('fraction')\n",
    "ax[0].set_ylabel('N')\n",
    "\n",
    "full_array = np.array(full_array)\n",
    "full_array = full_array[np.where(np.array(full_array) > 0)]\n",
    "\n",
    "ax[1].hist(full_array, bins = 20)\n",
    "median_fraction_gaining, one_sig_fraction_gaining, three_sig_fraction_gaining = np.percentile(full_array, [50, 84.1, 99.8])\n",
    "ax[1].axvline(median_fraction_gaining, color = colors[1])\n",
    "ax[1].axvline(one_sig_fraction_gaining, linestyle = 'dashed', color = colors[1])\n",
    "ax[1].axvline(three_sig_fraction_gaining, linestyle = 'dotted', color = colors[1])\n",
    "print(median_fraction_gaining, one_sig_fraction_gaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37fed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "full_array = []\n",
    "for j, key in enumerate(categories_names):\n",
    "    full_array.extend(results_df[key + ' gained density'].to_list())\n",
    "    ax[0].hist(results_df[key + ' gained density'], label = key, alpha = 0.5, color = colors[j], bins = np.arange(11)/10.)\n",
    "    ax[0].hist(results_df[key + ' gained density'], histtype = 'step', color = colors[j], linewidth = 2, bins = np.arange(11)/10.)\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('density')\n",
    "ax[0].set_ylabel('N')\n",
    "\n",
    "full_array = np.array(full_array)\n",
    "full_array = full_array[np.where(np.array(full_array) > 0)]\n",
    "\n",
    "ax[1].hist(full_array, bins = np.arange(11)/10.)\n",
    "median_density_gained, one_sig_density_gained, three_sig_density_gained = np.percentile(full_array, [50, 84.1, 99.8])\n",
    "ax[1].axvline(median_density_gained, color = colors[1])\n",
    "ax[1].axvline(one_sig_density_gained, linestyle = 'dashed', color = colors[1])\n",
    "ax[1].axvline(three_sig_density_gained, linestyle = 'dotted', color = colors[1])\n",
    "print(median_density_gained, one_sig_density_gained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b816b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "full_array = []\n",
    "for j, key in enumerate(categories_names):\n",
    "    full_array.extend(results_df[key + ' gaining density'].to_list())\n",
    "    ax[0].hist(results_df[key + ' gaining density'], label = key, alpha = 0.5, color = colors[j], bins = np.arange(11)/10.)\n",
    "    ax[0].hist(results_df[key + ' gaining density'], histtype = 'step', color = colors[j], linewidth = 2, bins = np.arange(11)/10.)\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('density')\n",
    "ax[0].set_ylabel('N')\n",
    "\n",
    "full_array = np.array(full_array)\n",
    "full_array = full_array[np.where(np.array(full_array) > 0)]\n",
    "\n",
    "ax[1].hist(full_array, bins = np.arange(11)/10.)\n",
    "median_density_gaining, one_sig_density_gaining, three_sig_density_gaining = np.percentile(full_array, [50, 84.1, 99.8])\n",
    "ax[1].axvline(median_density_gaining, color = colors[1])\n",
    "ax[1].axvline(one_sig_density_gaining, linestyle = 'dashed', color = colors[1])\n",
    "ax[1].axvline(three_sig_density_gaining, linestyle = 'dotted', color = colors[1])\n",
    "print(median_density_gaining, one_sig_density_gaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f38c259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the numbers of answers that have a given category above some threshold\n",
    "# there's probably a faster way to do this!\n",
    "category_numbers_gained = {}\n",
    "category_numbers_gaining = {}\n",
    "for j, key in enumerate(categories_names):\n",
    "    category_numbers_gained[key] = 0\n",
    "    category_numbers_gaining[key] = 0\n",
    "\n",
    "# not sure what the best numbers are here\n",
    "frac_threshold_gained = one_sig_fraction_gained\n",
    "dens_threshold_gained = one_sig_density_gained\n",
    "for key in categories_names:\n",
    "    for i, row in results_df.iterrows():\n",
    "        if ((row[key + ' gained'] > frac_threshold_gained) or (row[key + ' gained density'] > dens_threshold_gained)):\n",
    "            category_numbers_gained[key] += 1\n",
    "                \n",
    "\n",
    "# not sure what the best numbers are here\n",
    "frac_threshold_gaining = one_sig_fraction_gaining\n",
    "dens_threshold_gaining = one_sig_density_gaining\n",
    "for key in categories_names:\n",
    "    for i, row in results_df.iterrows():\n",
    "        if ((row[key + ' gaining'] > frac_threshold_gaining) or (row[key + ' gaining density'] > dens_threshold_gaining)):\n",
    "            category_numbers_gaining[key] += 1\n",
    "\n",
    "print(category_numbers_gained)\n",
    "print(category_numbers_gaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b25b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "f,ax = plt.subplots()\n",
    "data_gained = np.array(list(category_numbers_gained.values()))\n",
    "data_gaining = np.array(list(category_numbers_gaining.values()))\n",
    "srt_gained = np.argsort(data_gained)\n",
    "\n",
    "x = np.arange(len(categories_names))\n",
    "rects_gained = ax.barh(x + 0.2, data_gained[srt_gained], height = 0.4, \n",
    "                       tick_label = categories_names[srt_gained], color = colors[0], label = 'Gained Confidence')\n",
    "rects_gaining = ax.barh(x - 0.2, data_gaining[srt_gained], height = 0.4, \n",
    "                        tick_label = categories_names[srt_gained], color = colors[1], label = 'Still Gaining Confidence')\n",
    "#ax.set_xlim(0,60)\n",
    "ax.legend(loc = 'lower center')\n",
    "\n",
    "# remove all the axes, ticks and lower x label\n",
    "# aoff = ['right', 'left', 'top', 'bottom']\n",
    "aoff = ['right', 'top', 'bottom']\n",
    "for x in aoff:\n",
    "    ax.spines[x].set_visible(False)\n",
    "ax.tick_params(length=0)\n",
    "ax.set_xticklabels([' ']*len(data_gained))\n",
    "\n",
    "for r in rects_gained:\n",
    "    h = r.get_height()\n",
    "    w = r.get_width()\n",
    "    y = r.get_y()\n",
    "    ax.text(w + 5, y + 0.1, w, ha = 'center', va = 'bottom', zorder = 3, color = colors[0]) \n",
    "for r in rects_gaining:\n",
    "    h = r.get_height()\n",
    "    w = r.get_width()\n",
    "    y = r.get_y()\n",
    "    ax.text(w + 5, y + 0.1, w, ha = 'center', va = 'bottom', zorder = 3, color = colors[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eb7474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the script\n",
    "\n",
    "with open('data/category_words.json') as json_file:\n",
    "    categories_all = json.load(json_file)\n",
    "    \n",
    "# collapse the subcategories\n",
    "categories_primary = {}\n",
    "for k1 in categories_all:\n",
    "    categories_primary[k1] = categories_all[k1]['text']\n",
    "    for k2 in categories_all[k1]['subcategories']:\n",
    "        categories_primary[k1] += ' ' + categories_all[k1]['subcategories'][k2]\n",
    "\n",
    "# sheet name for this analysis, containing responses to one question\n",
    "results_df, category_numbers_gained, category_numbers_gaining = separate_into_predefined_categories(\n",
    "                categories_dict = categories_primary, \n",
    "                filename = 'data/ITP_CourseArtifacts_June 2021_END_of_Course_DeIDENTIFIED.xlsx', \n",
    "                sheet = 'Course Meta SelfEff', \n",
    "                column_number = 1, \n",
    "                additional_stopwords = [''], \n",
    "                wlen = 3, \n",
    "                figname = \"categories_histogram_CourseMetaSelfEff_primary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2040c10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run all the subcategories\n",
    "categories_sub = {}\n",
    "for k1 in categories_all:\n",
    "    for k2 in categories_all[k1]['subcategories']:\n",
    "        categories_sub[k2] = categories_all[k1]['subcategories'][k2]\n",
    "\n",
    "# sheet name for this analysis, containing responses to one question\n",
    "results_df, category_numbers_gained, category_numbers_gaining = separate_into_predefined_categories(\n",
    "                categories_dict = categories_sub, \n",
    "                filename = 'data/ITP_CourseArtifacts_June 2021_END_of_Course_DeIDENTIFIED.xlsx', \n",
    "                sheet = 'Course Meta SelfEff', \n",
    "                column_number = 1, \n",
    "                additional_stopwords = [''], \n",
    "                wlen = 3, \n",
    "                figname = \"categories_histogram_CourseMetaSelfEff_subcategories.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05f0e12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
