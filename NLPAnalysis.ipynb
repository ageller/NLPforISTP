{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8c552a5",
   "metadata": {},
   "source": [
    "# NLP analysis\n",
    "\n",
    "```\n",
    "conda create --name NLP -c conda-forge python=3.10 jupyter pandas numpy matplotlib openpyxl nltk gensim pyldavis\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e87faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you are running this for the first time on a new installation, uncomment below and run this cell\n",
    "## (This only needs to be run once.)\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b589ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b3efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import my code (and set to autoreload <-- only necessary while coding/debuggin)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from NLPforISP import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80225857",
   "metadata": {},
   "source": [
    "## Read in the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68559a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full data file with multiple sheets\n",
    "filename = 'data/ITP_CourseArtifacts_June 2021_END_of_Course_DeIDENTIFIED.xlsx'\n",
    "\n",
    "# sheet name for this analysis, containing responses to one question\n",
    "sheet = 'Course Meta SelfEff'\n",
    "\n",
    "df = pd.read_excel(filename, sheet)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986b425d",
   "metadata": {},
   "source": [
    "## Get the bigrams and trigrams and create bar charts of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37de48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add appropriate words that will be ignored in the analysis\n",
    "additional_stopwords = ['1', '2', 'one', 'two', 'etc']\n",
    "\n",
    "# get a string of the words contained in all the answers from this DataFrame\n",
    "string_of_answers = getStringOfWords(df, 1)\n",
    "\n",
    "# get the bigrams and trigrams\n",
    "bigrams = getNgrams(string_of_answers, 2, additional_stopwords = additional_stopwords)\n",
    "trigrams = getNgrams(string_of_answers, 3, additional_stopwords = additional_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3c5e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a plot of the bigrams and trigrams\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "N = 20\n",
    "plotNgrams(bigrams, N, ax = ax1)\n",
    "plotNgrams(trigrams, N, ax = ax2)\n",
    "_ = ax1.set_title(str(N) + ' Most Frequently Occuring Bigrams')\n",
    "_ = ax2.set_title(str(N) + ' Most Frequently Occuring Trigrams')\n",
    "plt.subplots_adjust(wspace = 0.6, left = 0.15, right = 0.99, top = 0.95, bottom = 0.07)\n",
    "\n",
    "plt.savefig('ngrams.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ee5820",
   "metadata": {},
   "source": [
    "## Topic modeling\n",
    "\n",
    "Using NLTK + gensim,  Latent Dirichlet Allocation (LDA) algorithm, which uses unsupervised learning to extract the main topics (i.e., a set of words) that occur in a collection of text samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d680a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the topic modeling (which also generates a \"dictionary\" and a \"bag of words\")\n",
    "dictionary, bow_corpus, lda_model, perplexity, coherence = runLDATopicModel(df, 1, 5, workers = 2, \n",
    "      additional_stopwords = additional_stopwords, no_below = 15, no_above = 1, keep_n = int(1e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c0f461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dictionary\n",
    "printDictionary(dictionary, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e19cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the bag of words\n",
    "printBagOfWords(dictionary, bow_corpus, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6672d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the topic model\n",
    "printTopicModel(lda_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0941b43",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "Run a series of LDA models and plot the coherence and perplexity scores to try to identify the optimal number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083bf0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = np.arange(10) + 1\n",
    "dictionary, bow_corpus, lda_model, perplexity, coherence = runLDATopicModel(df, 1, num_topics, workers = 2, \n",
    "      additional_stopwords = additional_stopwords, no_below = 15, no_above = 1, keep_n = int(1e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb18609b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the index of the best model by selecting the maximum coherence score\n",
    "best_index = np.argmax(coherence)\n",
    "num_topics[best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3c32bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the results\n",
    "# higher coherence is better\n",
    "# lower perplexity is better\n",
    "\n",
    "f, (ax1, ax2) = plotLDAMetrics(num_topics, coherence, perplexity, best_index)\n",
    "plt.savefig('metrics.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e622e195",
   "metadata": {},
   "source": [
    "## Visualization using pyLDAvis\n",
    "\n",
    "- https://nbviewer.org/github/bmabey/hacker_news_topic_modelling/blob/master/HN%20Topic%20Model%20Talk.ipynb\n",
    "- https://github.com/bmabey/pyLDAvis\n",
    "- https://nbviewer.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb\n",
    "\n",
    "Most of the visualization is self expanatory, but the slider to adjust the \"relevant metric\" takes some reading. \n",
    "From here: https://we1s.ucsb.edu/research/we1s-tools-and-software/topic-model-observatory/tmo-guide/tmo-guide-pyldavis/\n",
    "\n",
    "\"A “relevance metric” slider scale at the top of the right panel controls how the words for a topic are sorted. As defined in the article by Sievert and Shirley (the creators of LDAvis, on which pyLDAvis is based), “relevance” combines two different ways of thinking about the degree to which a word is associated with a topic:\n",
    "\n",
    "On the one hand, we can think of a word as highly associated with a topic if its frequency in that topic is high. By default the lambda (λ) value in the slider is set to “1,” which sorts words by their frequency in the topic (i.e., by the length of their red bars).\n",
    "\n",
    "On the other hand, we can think of a word as highly associated with a topic if its “lift” is high. “Lift”–a term that Sievert and Shirley borrow from research on topic models by others–means basically how much a word’s frequency sticks out in a topic above the baseline of its overall frequency in the model (i.e., the “the ratio of a term’s probability within a topic to its marginal probability across the corpus,” or the ratio between its red bar and blue bar).\n",
    "\n",
    "By default, pyLDAvis is set for λ = 1, which sorts words just by their frequency within the specific topic (by their red bars).  By contrast, setting λ = 0 words sorts words by their “lift. This means that words whose red bars are nearly as long as their blue bars will be sorted at the top. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124c5964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baf6cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3af8ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Note: I chose the best index from the lda_models array while plotting the coherence and perplexity metrics\n",
    "pyLDAvis.gensim_models.prepare(lda_model[best_index], bow_corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f34c99",
   "metadata": {},
   "source": [
    "#  TODO\n",
    "\n",
    "\n",
    "## Try Mallet LDA?\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/ <-- this also contains some great additional steps to check out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943c8548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072469ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
