{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8c552a5",
   "metadata": {},
   "source": [
    "# Initial NLP analysis\n",
    "\n",
    "```\n",
    "conda create --name NLP -c conda-forge python=3.10 jupyter pandas numpy matplotlib openpyxl nltk gensim pyldavis\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c903c5a6",
   "metadata": {},
   "source": [
    "# Eventually I should write this as a .py file and import functions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b589ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68559a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full data file with multiple sheets\n",
    "filename = 'data/ITP_CourseArtifacts_June 2021_END_of_Course_DeIDENTIFIED.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9070b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sheet name for this analysis, containing responses to one question\n",
    "sheet = 'Course Meta SelfEff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7685bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(filename, sheet)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8fa03f",
   "metadata": {},
   "source": [
    "## Look for n-grams\n",
    "\n",
    "- NLTK (followed this): https://towardsdatascience.com/from-dataframe-to-n-grams-e34e29df3460\n",
    "- textBlob (haven't tried) : https://levelup.gitconnected.com/simple-nlp-in-python-f5196db63aff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77b41b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e87faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this only needs to be run once\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d585ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add appropriate words that will be ignored in the analysis\n",
    "additional_stopwords = ['1', '2', 'one', 'two', 'etc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb209744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, additional_stopwords = [''], wlen = 3, stem = True, ):\n",
    "    \"\"\"\n",
    "    A simple function to clean up the data. All the words that\n",
    "    are not designated as a stop word is then lemmatized and (optionally) stemmed\n",
    "    after encoding and basic regex parsing are performed.\n",
    "    \n",
    "    originally from here : https://towardsdatascience.com/from-dataframe-to-n-grams-e34e29df3460\n",
    "    with modifications by AMG\n",
    "    \"\"\"\n",
    "    \n",
    "    # define the lemmatizer, stemmer and stopwords\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    #stemmer = nltk.stem.PorterStemmer()\n",
    "    stopwords = nltk.corpus.stopwords.words('english') + additional_stopwords\n",
    "    \n",
    "    # initial simple regex parsing and create a list of words\n",
    "    text = (unicodedata.normalize('NFKD', text)\n",
    "        .encode('ascii', 'ignore')\n",
    "        .decode('utf-8', 'ignore')\n",
    "        .lower())\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "    \n",
    "    # pass through the lemmatizer and (optionally) the stemmer\n",
    "    processed = []\n",
    "    for word in words:\n",
    "        if (word not in stopwords and len(word) > wlen):\n",
    "            w = wnl.lemmatize(word)\n",
    "            #print(word, wnl.lemmatize(word), stemmer.stem(word), stemmer.stem(w))\n",
    "            if (stem):# and not w.endswith('e')):\n",
    "                processed.append(stemmer.stem(w))\n",
    "            else:\n",
    "                processed.append(w)\n",
    "    \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be66cacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "string_of_words = ''.join(str(original_words))\n",
    "print(preprocess(string_of_words, additional_stopwords = additional_stopwords, stem = True))\n",
    "print(preprocess(string_of_words, additional_stopwords = additional_stopwords, stem = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d002872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the words in order (excluding the stop words)\n",
    "\n",
    "# 1. convert the answers column to a list\n",
    "list_of_answers = df[df.columns[1]].tolist() \n",
    "\n",
    "# 2. convert that list to a long string\n",
    "string_of_answers = ''.join(str(list_of_answers))\n",
    "\n",
    "# 3. run through the \"preprocess\" function that will return a list of (lemmatized) words\n",
    "# SHOULD I STEM FIRST??\n",
    "processed_words = preprocess(string_of_answers, additional_stopwords = additional_stopwords, stem = True)\n",
    "processed_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2991bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the bigrams\n",
    "bigrams = pd.Series(nltk.ngrams(processed_words, 2)).value_counts()\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d33868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the trigrams\n",
    "trigrams = pd.Series(nltk.ngrams(processed_words, 3)).value_counts()\n",
    "trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b60eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "\n",
    "N = 20\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "\n",
    "ind = np.arange(N)\n",
    "\n",
    "bigrams_plot = bigrams[0:N].sort_values()\n",
    "ax1.barh(ind, bigrams_plot, 0.9, color = 'gray')\n",
    "ax1.set_yticks(ind)\n",
    "_ = ax1.set_yticklabels(bigrams_plot.index.str.join(sep=' '))\n",
    "_ = ax1.set_title(str(N) + ' Most Frequently Occuring Bigrams')\n",
    "_ = ax1.set_xlabel('# of Occurances')\n",
    "\n",
    "trigrams_plot = trigrams[0:N].sort_values()\n",
    "ax2.barh(ind, trigrams_plot, 0.9, color = 'gray')\n",
    "ax2.set_yticks(ind)\n",
    "_ = ax2.set_yticklabels(trigrams_plot.index.str.join(sep=' '))\n",
    "_ = ax2.set_title(str(N) + ' Most Frequently Occuring Trigrams')\n",
    "_ = ax2.set_xlabel('# of Occurances')\n",
    "\n",
    "plt.subplots_adjust(wspace = 0.9, left = 0.15, right = 0.99, top = 0.95, bottom = 0.07)\n",
    "\n",
    "plt.savefig('ngrams.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ee5820",
   "metadata": {},
   "source": [
    "## Topic modeling\n",
    "\n",
    "- NLTK and gensim : https://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925\n",
    "- NLTK and gensim : https://towardsdatascience.com/introduction-to-nlp-part-5b-unsupervised-topic-model-in-python-ab04c186f295\n",
    "- pyLDAvis : https://www.projectpro.io/article/10-nlp-techniques-every-data-scientist-should-know/415#toc-10\n",
    "- pyLDAvis : https://neptune.ai/blog/pyldavis-topic-modelling-exploration-tool-that-every-nlp-data-scientist-should-know"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3f9570",
   "metadata": {},
   "source": [
    "Trying NLTK + gensim,  Latent Dirichlet Allocation (LDA) algorithm, which uses unsupervised learning to extract the main topics (i.e., a set of words) that occur in a collection of text samples.  The first link above has a very good general explanation of the method, and [here's the Jupyter notebook on their github repo](https://github.com/priya-dwivedi/Deep-Learning/blob/master/topic_modeling/LDA_Newsgroup.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b63c453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess each answer separatley\n",
    "processed_answers = []\n",
    "for answer in list_of_answers:\n",
    "    processed_answers.append(preprocess(answer, additional_stopwords = additional_stopwords))\n",
    "processed_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c143ffdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to a \"bag of words\"\n",
    "dictionary = gensim.corpora.Dictionary(processed_answers)\n",
    "\n",
    "# filter (optional)\n",
    "# remove words appearing less than no_below times\n",
    "# remove words appearing in more than no_above (fraction) of all documents\n",
    "no_below = 15\n",
    "no_above = 1 # don't use this\n",
    "keep_n = int(1e5)\n",
    "dictionary.filter_extremes(no_below = no_below, no_above = no_above, keep_n = keep_n)\n",
    "\n",
    "# Create the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many\n",
    "# words and how many times those words appear. Save this to 'bow_corpus'\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d8e28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking dictionary created\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a2bf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the bag of words\n",
    "answer_num = 0\n",
    "bow_answer = bow_corpus[answer_num]\n",
    "\n",
    "for i in range(len(bow_answer)):\n",
    "    print(f'Word {bow_answer[i][0]} (\"{dictionary[bow_answer[i][0]]}\") appears {bow_answer[i][1]} time.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37b2c39",
   "metadata": {},
   "source": [
    "## Running LDA using Bag of Word\n",
    "\n",
    "*This markdown is taken directly from [here](https://github.com/priya-dwivedi/Deep-Learning/blob/master/topic_modeling/LDA_Newsgroup.ipynb).*\n",
    "\n",
    "**We will be running LDA using multiple CPU cores to parallelize and speed up model training.**\n",
    "\n",
    "Some of the parameters we will be tweaking are:\n",
    "\n",
    "* **num_topics** is the number of requested latent topics to be extracted from the training corpus.\n",
    "* **id2word** is a mapping from word ids (integers) to words (strings). It is used to determine the vocabulary size, as well as for debugging and topic printing.\n",
    "* **workers** is the number of extra processes to use for parallelization. Uses all available cores by default.\n",
    "* **alpha** and **eta** are hyperparameters that affect sparsity of the document-topic (theta) and topic-word (lambda) distributions. We will let these be the default values for now(default value is `1/num_topics`)\n",
    "    - Alpha is the per document topic distribution.\n",
    "        * High alpha: Every document has a mixture of all topics(documents appear similar to each other).\n",
    "        * Low alpha: Every document has a mixture of very few topics\n",
    "\n",
    "    - Eta is the per topic word distribution.\n",
    "        * High eta: Each topic has a mixture of most words(topics appear similar to each other).\n",
    "        * Low eta: Each topic has a mixture of few words.\n",
    "* **passes** is the number of training passes through the corpus. \n",
    "\n",
    "Documentation here: https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "\n",
    "# I NEED TO CHECK ALL THESE PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e9c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA mono-core -- fallback code in case LdaMulticore throws an error on your machine\n",
    "# lda_model = gensim.models.LdaModel(bow_corpus, \n",
    "#                                    num_topics = 5, \n",
    "#                                    id2word = dictionary,                                    \n",
    "#                                    passes = 50)\n",
    "\n",
    "# Train your lda model using gensim.models.LdaMulticore \n",
    "lda_model =  gensim.models.LdaMulticore(\n",
    "    bow_corpus, \n",
    "    num_topics = 8, \n",
    "    id2word = dictionary,  \n",
    "    passes = 20,\n",
    "    workers = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eac55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each topic, explore the words occuring in that topic and its relative weight\n",
    "# Then a human would need to give each topic a name (or \"theme\")\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f'Topic: {idx}\\nWords: {topic}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45f917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "# I need to look up the args here as well\n",
    "coherence_model_lda = gensim.models.coherencemodel.CoherenceModel(\n",
    "    model = lda_model, \n",
    "    texts = processed_answers, \n",
    "    dictionary = dictionary, \n",
    "    coherence = 'c_v'\n",
    ")\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('gensim coherence score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f34c99",
   "metadata": {},
   "source": [
    "## Optimization (todo)\n",
    "\n",
    "I could calculate the coherence score with various changes in the args for the lda model (e.g., number of topics), and then pick the one with the best coherence score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e622e195",
   "metadata": {},
   "source": [
    "## Visualization using pyLDAvis\n",
    "\n",
    "- https://nbviewer.org/github/bmabey/hacker_news_topic_modelling/blob/master/HN%20Topic%20Model%20Talk.ipynb\n",
    "- https://github.com/bmabey/pyLDAvis\n",
    "- https://nbviewer.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb\n",
    "\n",
    "Most of the visualization is self expanatory, butthe slider to adjust the \"relevant metric\" takes some reading. \n",
    "From here: https://we1s.ucsb.edu/research/we1s-tools-and-software/topic-model-observatory/tmo-guide/tmo-guide-pyldavis/\n",
    "\n",
    "\"A “relevance metric” slider scale at the top of the right panel controls how the words for a topic are sorted. As defined in the article by Sievert and Shirley (the creators of LDAvis, on which pyLDAvis is based), “relevance” combines two different ways of thinking about the degree to which a word is associated with a topic:\n",
    "\n",
    "On the one hand, we can think of a word as highly associated with a topic if its frequency in that topic is high. By default the lambda (λ) value in the slider is set to “1,” which sorts words by their frequency in the topic (i.e., by the length of their red bars).\n",
    "\n",
    "On the other hand, we can think of a word as highly associated with a topic if its “lift” is high. “Lift”–a term that Sievert and Shirley borrow from research on topic models by others–means basically how much a word’s frequency sticks out in a topic above the baseline of its overall frequency in the model (i.e., the “the ratio of a term’s probability within a topic to its marginal probability across the corpus,” or the ratio between its red bar and blue bar).\n",
    "\n",
    "By default, pyLDAvis is set for λ = 1, which sorts words just by their frequency within the specific topic (by their red bars).  By contrast, setting λ = 0 words sorts words by their “lift. This means that words whose red bars are nearly as long as their blue bars will be sorted at the top. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124c5964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baf6cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3af8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.gensim_models.prepare(lda_model, bow_corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072469ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
